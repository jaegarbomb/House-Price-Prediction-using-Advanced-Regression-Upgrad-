{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from warnings import filterwarnings\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting certain variables for the program.\n",
    "seed = 42\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing redundant features:\n",
    "def redundant_features(master):\n",
    "    redundants = []\n",
    "    for i in master.columns:\n",
    "        counts = master[i].value_counts()\n",
    "        count_max = counts.iloc[0]\n",
    "        if count_max / len(master) * 100 > 98: # if there is one value more than 98% in the entire dataset i.e. biased.\n",
    "            redundants.append(i)\n",
    "    redundants = list(redundants)\n",
    "    return redundants\n",
    "\n",
    "def drop_outliers(x):    \n",
    "    for col in outl_col:\n",
    "        Q1 = x[col].quantile(.25)\n",
    "        Q3 = x[col].quantile(.99)\n",
    "        IQR = Q3-Q1\n",
    "        x =  x[(x[col] >= (Q1-(1.5*IQR))) & (x[col] <= (Q3+(1.5*IQR)))] \n",
    "    return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = pd.read_csv(\"train.csv\")\n",
    "master = master.drop([\"Id\"],axis=1)\n",
    "print(master.shape)\n",
    "master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking Missing Data\n",
    "master.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the percentage of Null values in all the columns\n",
    "print('Percentage of Missing Values in each column is as follows:')\n",
    "print(round(master.isnull().sum()/len(master.index)*100,2).sort_values(ascending=False)[ round(master.isnull().sum()/len(master.index),2) > 0 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Obs</b>:</br>\n",
    "5 features have large number missing values. Keeping a <b> arbitary threshold value of 80%</b>, the top four columns are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = [\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\"]\n",
    "master = master.drop(cols_drop,axis=1)\n",
    "master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some columns have numerical values but have categorical meanings. Thus these needs to be ordered into categorical data type.\n",
    "cols_cat = [\"MSSubClass\",\"OverallQual\",\"OverallCond\",\"MoSold\"]\n",
    "\n",
    "for col in cols_cat:\n",
    "    master[col] = master[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing the missing data with assumptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master['FireplaceQu'] = master['FireplaceQu'].fillna('No_Fireplace')\n",
    "master['GarageYrBlt'] = master['GarageYrBlt'].fillna(0)\n",
    "master['MasVnrType'] = master['MasVnrType'].fillna('None')\n",
    "master['MasVnrArea'] = master['MasVnrArea'].fillna(0)\n",
    "master['MasVnrArea'] = master['MasVnrArea'].fillna(0)\n",
    "\n",
    "#NA = No Garage (assumed)\n",
    "master['GarageCond'] = master['GarageCond'].fillna('None')\n",
    "master['GarageType'] = master['GarageType'].fillna('None')\n",
    "master['GarageFinish'] = master['GarageFinish'].fillna('None')\n",
    "master['GarageQual'] = master['GarageQual'].fillna('None')  \n",
    "\n",
    "#NA = No Basement (assumed)\n",
    "master['BsmtExposure'] = master['BsmtExposure'].fillna('None')\n",
    "master['BsmtFinType2'] = master['BsmtFinType2'].fillna('None')\n",
    "master['BsmtCond'] = master['BsmtCond'].fillna('None')\n",
    "master['BsmtQual'] = master['BsmtQual'].fillna('None')\n",
    "master['BsmtFinType1'] = master['BsmtFinType1'].fillna('None')\n",
    "\n",
    "#LotFrontage : Replacing Null value with the median of the neighbourhood\n",
    "master['LotFrontage'] = master.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Filling the Electrical rows with the mode\n",
    "master['Electrical'] = master['Electrical'].fillna(master['Electrical'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Features vs Sales Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Categorical Features with Sale Price\n",
    "def facetgrid_boxplot(x, y, **kwargs):\n",
    "    sns.boxplot(x=x, y=y)\n",
    "    x=plt.xticks(rotation=90)\n",
    "    \n",
    "categorical = master.select_dtypes(exclude=['int64','float64'])\n",
    "f = pd.melt(master, id_vars=['SalePrice'], value_vars=sorted(master[categorical.columns]))\n",
    "g = sns.FacetGrid(f, col=\"variable\", col_wrap=3, sharex=False, sharey=False)\n",
    "g = g.map(facetgrid_boxplot, \"value\", \"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the observations from above plot:\n",
    "- Paved alleys properties has higher price.\n",
    "- Houses where the basement quality is good and excellent are sold at higher prices.\n",
    "- Houses with good and excellent garages are sold at higher prices.\n",
    "- Houses with good quality kitchens has better prices.\n",
    "- Houses with gas heating has good prices as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe of numerical features:\n",
    "master_num = master.select_dtypes(include=['int64','float64'])\n",
    "master_num.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising numerical predictor variables with Target Variables\n",
    "fig,axs= plt.subplots(11,3,figsize=(20,80))\n",
    "for i,ax in zip(master_num.columns,axs.flatten()):\n",
    "    sns.scatterplot(x=i, y='SalePrice', hue='SalePrice',data=master_num,ax=ax,palette='vlag')\n",
    "    plt.xlabel(i,fontsize=12)\n",
    "    plt.ylabel('SalePrice',fontsize=12)\n",
    "    ax.set_title('SalePrice'+' VS '+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting heatmap of numerical features\n",
    "plt.subplots(figsize = (25,20))\n",
    "corr = master_num.corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.heatmap(round(corr,2), cmap='coolwarm' ,mask=mask, annot=True, center = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(master.isnull().sum()/len(master.index)*100,2).sort_values(ascending=False)[round(master.isnull().sum()/len(master.index),2) > 0 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While doing EDA, it was noticed that certain categorical features represented over 95% of the dataset. This bias will not help in proper modelling. Thus it is optimal that these features be removed. <br>\n",
    "\n",
    "While testing out the function it was noticed that 95% gave out 10 features whereas 98 % gave 6  features to remove. <br>\n",
    "\n",
    "Thus it was arbitarily decided to use 98% as threshold. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_features = redundant_features(master)\n",
    "print(redundant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = master.drop(redundant_features,axis=1)\n",
    "master.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From EDA it was seen that Living Area, Garage Area, Basement Area and Lot Area. Removing outliers from these. Other outliers will be handled  during power transform.\n",
    "outl_col = ['GrLivArea','GarageArea','TotalBsmtSF','LotArea'] \n",
    "master = drop_outliers(master)\n",
    "master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating some new features based on the existing features\n",
    "\n",
    "master['YrBltAndRemod']=master['YearBuilt']+master['YearRemodAdd']\n",
    "\n",
    "# Overall area:\n",
    "master['Total_sqr_footage'] = (master['BsmtFinSF1'] + master['BsmtFinSF2'] + master['1stFlrSF'] + master['2ndFlrSF'])\n",
    "\n",
    "# Total number of bathrooms:\n",
    "master['Total_Bathrooms'] = (master['FullBath'] + (0.5 * master['HalfBath']) + master['BsmtFullBath'] + (0.5 * master['BsmtHalfBath']))\n",
    "\n",
    "# Total porch area \n",
    "master['Total_porch_sf'] = (master['OpenPorchSF'] + master['EnclosedPorch'] + master['ScreenPorch'] + master['WoodDeckSF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modelling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dummy Variables for Categorical Columns\n",
    "num_col=[]\n",
    "cat_col=[]\n",
    "\n",
    "for i in master.columns:\n",
    "    if master[i].dtypes in [\"int64\",\"float64\"]:\n",
    "        num_col.append(i)\n",
    "    else:\n",
    "        cat_col.append(i)\n",
    "df = pd.get_dummies(master[cat_col],drop_first=True)\n",
    "master=pd.concat([master,df],axis=1)\n",
    "master= master.drop(cat_col,axis=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor Variables:\n",
    "X = master.drop('SalePrice',axis=1)\n",
    "\n",
    "# Target Variable\n",
    "y = master['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation coefficient threshold is arbitarily taken  as 0.7\n",
    "threshold  = 0.7\n",
    "\n",
    "# Checking co-related features\n",
    "corr = X.corr()\n",
    "\n",
    "corr1 = corr[abs(corr)>= threshold] \n",
    "corr2 =  corr.where(~np.tril(np.ones(corr.shape)).astype(np.bool))  #To remove repetition and 1 correlations\n",
    "\n",
    "corr_result = corr2.stack()\n",
    "print(corr_result[(abs(corr_result) > threshold)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_drop = [\"YearBuilt\",\"YearRemodAdd\",\"BsmtFinSF2\",\"TotalBsmtSF\",\"2ndFlrSF\",\"GarageQual_TA\",\"GarageCond_None\",\"SaleType_New\",\"SaleType_WD\"]\n",
    "X.drop(col_drop,axis=1,inplace=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of Target Variable\n",
    "sns.distplot(y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This datat is slightly right skewed. Thus a Power Transform to convert this to a Gaussian normal curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the Target feature to make the data gaussian\n",
    "pt = PowerTransformer(method='box-cox', standardize=False)\n",
    "y_train = pt.fit_transform(y_train.to_frame())\n",
    "y_test = pt.transform(y_test.to_frame())\n",
    "\n",
    "sns.distplot(y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling: <br>\n",
    "\n",
    "Using a MinMax scaler, the features will be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_train.columns = X.columns\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_test.columns = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression (L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of alphas to tune\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n",
    " 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n",
    " 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n",
    "ridge = Ridge()\n",
    "\n",
    "# Using RFE to find top 300 variables\n",
    "rfe = RFE(estimator=Ridge(), n_features_to_select=300)\n",
    "rfe = rfe.fit(X_train,y_train)\n",
    "col = X_train.columns[rfe.support_]\n",
    "X_train_rfe = X_train[col]\n",
    "X_test_rfe = X_test[col]\n",
    "\n",
    "# cross validation\n",
    "folds = 10\n",
    "model_cv = GridSearchCV(estimator = ridge, param_grid = params, scoring= 'r2', cv = folds, return_train_score=True, verbose = 1)            \n",
    "model_cv.fit(X_train_rfe, y_train) \n",
    "\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results = cv_results[cv_results['param_alpha']<=30]\n",
    "# plotting mean test and train scoes with alpha \n",
    "cv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n",
    "\n",
    "# plotting\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.title(\"R2 Score and Alpha\")\n",
    "plt.legend(['train score', 'test score'], loc='upper right')\n",
    "plt.xticks(np.arange(0,30,5))\n",
    "plt.show()\n",
    "\n",
    "alpha = cv_results['param_alpha'].loc[cv_results['mean_test_score'].idxmax()]\n",
    "print('The optimum alpha is',alpha)\n",
    "ridge_final = Ridge(alpha=alpha)\n",
    "ridge_final.fit(X_train_rfe,y_train)\n",
    "ridge_coef = ridge_final.coef_\n",
    "y_test_pred = ridge_final.predict(X_test_rfe)\n",
    "print('The R2 Score of the model on the test dataset for optimum alpha is',r2_score(y_test, y_test_pred))\n",
    "print('The MSE of the model on the test dataset for optimum alpha is', mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USing VIF to remove  features:\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_rfe.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "high_vif = vif[vif['VIF']>10]\n",
    "high_vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though Total Square Footage has a high VIF, it is one of the most important features to predict the cost.\n",
    "tot_sq_index = high_vif.loc[high_vif.Features == \"Total_sqr_footage\"].index.tolist()\n",
    "high_vif.drop(tot_sq_index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping cols with high VIF value.\n",
    "X_train_rfe2 = X_train_rfe.drop(high_vif.Features,axis=1)\n",
    "X_test_rfe2 = X_test_rfe.drop(high_vif.Features,axis=1)\n",
    "\n",
    "# This gives columns without Multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the second Ridge Model\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n",
    " 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n",
    " 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n",
    "ridge = Ridge(random_state=100)\n",
    "\n",
    "# cross validation\n",
    "folds = 10\n",
    "model_cv = GridSearchCV(estimator = ridge, param_grid = params, scoring= 'r2', cv = folds, return_train_score=True, verbose = 1)            \n",
    "model_cv.fit(X_train_rfe2, y_train) \n",
    "\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results = cv_results[cv_results['param_alpha']<=30]\n",
    "# plotting mean test and train scoes with alpha \n",
    "cv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n",
    "\n",
    "# plotting\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.title(\"R2 Score and Alpha\")\n",
    "plt.legend(['train score', 'test score'], loc='upper right')\n",
    "plt.xticks(np.arange(0,30,5))\n",
    "plt.show()\n",
    "\n",
    "alpha = cv_results['param_alpha'].loc[cv_results['mean_test_score'].idxmax()]\n",
    "print('The optimum alpha is',alpha)\n",
    "ridge_final2 = Ridge(alpha=alpha,random_state=100)\n",
    "ridge_final2.fit(X_train_rfe2,y_train)\n",
    "ridge_coef2 = ridge_final2.coef_\n",
    "y_test_pred = ridge_final2.predict(X_test_rfe2)\n",
    "print('The R2 Score of the model on the test dataset for optimum alpha is',r2_score(y_test, y_test_pred))\n",
    "print('The MSE of the model on the test dataset for optimum alpha is', mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coefficients of the Model:\n",
    "ridge_coeff2 = pd.DataFrame(np.atleast_2d(ridge_coef2),columns=X_train_rfe2.columns)\n",
    "ridge_coeff2 = ridge_coeff2.T\n",
    "ridge_coeff2.rename(columns={0: 'Ridge Co-Efficient'},inplace=True)\n",
    "ridge_coeff2.sort_values(by=['Ridge Co-Efficient'], ascending=False,inplace=True)\n",
    "ridge_coeff2.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above features are the top 20 features likely impacting the property value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression (L1 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model with an arbitrary alpha to understand the value ranges\n",
    "lasso1 = Lasso(alpha=0.0001)        \n",
    "lasso1.fit(X_train_rfe2, y_train) \n",
    "\n",
    "y_test_pred = lasso1.predict(X_test_rfe2)\n",
    "print('The R2 Score of the model on the test dataset for 0.0001 alpha is',r2_score(y_test, y_test_pred))\n",
    "print('The MSE of the model on the test dataset for optimum alpha is', mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Model with GridSearch CV to find the optimum alpha.\n",
    "\n",
    "params = {'alpha': [0.00001, 0.00009, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009 ]}\n",
    "lasso = Lasso(random_state=100)\n",
    "\n",
    "# cross validation\n",
    "folds = 10\n",
    "model_cv = GridSearchCV(estimator = lasso, param_grid = params, scoring= 'r2', cv = folds, return_train_score=True, verbose = 1)            \n",
    "model_cv.fit(X_train_rfe2, y_train) \n",
    "\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "# plotting\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.title(\"R2 Score and Alpha\")\n",
    "plt.legend(['train score', 'test score'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "alpha = cv_results['param_alpha'].loc[cv_results['mean_test_score'].idxmax()]\n",
    "print('The optimum alpha is',alpha)\n",
    "lasso_final2 = Lasso(alpha=alpha,random_state=100)\n",
    "lasso_final2.fit(X_train_rfe2,y_train)\n",
    "lasso_coef2 = lasso_final2.coef_\n",
    "y_test_pred = lasso_final2.predict(X_test_rfe2)\n",
    "print('The R2 Score of the model on the test dataset for optimum alpha is',r2_score(y_test, y_test_pred))\n",
    "print('The MSE of the model on the test dataset for optimum alpha is', mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Co-efficients of the model\n",
    "lasso_coeff2 = pd.DataFrame(np.atleast_2d(lasso_coef2),columns=X_train_rfe2.columns)\n",
    "lasso_coeff2 = lasso_coeff2.T\n",
    "lasso_coeff2.rename(columns={0: \"Lasso Co-Efficient\"},inplace=True)\n",
    "lasso_coeff2.sort_values(by=['Lasso Co-Efficient'], ascending=False,inplace=True)\n",
    "lasso_coeff2.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are the top 20 features from the Lasso model that can be used to predict the price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Ridge regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_final2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Lasso Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjective Questions Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. <br>\n",
    "What is the optimal value of alpha for ridge and lasso regression? <br>\n",
    "What will be the changes in the model if you choose double the value of alpha for both ridge and lasso? <br>\n",
    "What will be the most important predictor variables after the change is implemented?<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value of alpha for Ridge regression is <b>3</b> and Lasso is <b>0.0006</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Ridge Model by doubling the value of alpha to 6\n",
    "ridge_double = Ridge(alpha=6,random_state=seed)\n",
    "ridge_double.fit(X_train_rfe2,y_train)\n",
    "ridge_double_coef = ridge_double.coef_\n",
    "y_test_pred = ridge_double.predict(X_test_rfe2)\n",
    "\n",
    "print('The R2 Score of the model on the test dataset for doubled alpha is',r2_score(y_test, y_test_pred))\n",
    "print('The MSE of the model on the test dataset for doubled alpha is', mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "ridge_double_coeff = pd.DataFrame(np.atleast_2d(ridge_double_coef),columns=X_train_rfe2.columns)\n",
    "ridge_double_coeff = ridge_double_coeff.T\n",
    "ridge_double_coeff.rename(columns={0: 'Ridge Doubled Alpha Co-Efficient'},inplace=True)\n",
    "ridge_double_coeff.sort_values(by=['Ridge Doubled Alpha Co-Efficient'], ascending=False,inplace=True)\n",
    "\n",
    "print('\\n The most important predictor variables are as follows:')\n",
    "ridge_double_coeff.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Lasso Model by doubling the value of alpha to 0.0006\n",
    "lasso_double = Lasso(alpha=0.0006,random_state=seed)\n",
    "lasso_double.fit(X_train_rfe2,y_train)\n",
    "lasso_double_coef = lasso_double.coef_\n",
    "y_test_pred = lasso_double.predict(X_test_rfe2)\n",
    "print('The R2 Score of the model on the test dataset for doubled alpha is',r2_score(y_test, y_test_pred))\n",
    "print('The MSE of the model on the test dataset for doubled alpha is', mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "lasso_double_coeff = pd.DataFrame(np.atleast_2d(lasso_double_coef),columns=X_train_rfe2.columns)\n",
    "lasso_double_coeff = lasso_double_coeff.T\n",
    "lasso_double_coeff.rename(columns={0: 'Lasso Doubled Alpha Co-Efficient'},inplace=True)\n",
    "lasso_double_coeff.sort_values(by=['Lasso Doubled Alpha Co-Efficient'], ascending=False,inplace=True)\n",
    "\n",
    "print('\\n The most important predictor variables are as follows:')\n",
    "lasso_double_coeff.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. <br>\n",
    "After building the model, you realised that the five most important predictor variables in the lasso model are not available in the incoming data.<br>You will now have to create another model excluding the five most important predictor variables. Which are the five most important predictor variables now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the 5 most important predictor variables from the incoming dataset\n",
    "col_drop = [\"Total_sqr_footage\",\"GarageCars\",\"TotRmsAbvGrd\",\"Fireplaces\",\"GarageArea\"]\n",
    "\n",
    "X_test_rfe3 = X_test_rfe2.drop(col_drop,axis=1)\n",
    "X_train_rfe3 = X_train_rfe2.drop(col_drop,axis=1)\n",
    "\n",
    "# Building Lasso Model with the new dataset\n",
    "lasso3 = Lasso(alpha=0.0001,random_state=100)\n",
    "lasso3.fit(X_train_rfe3,y_train)\n",
    "lasso3_coef = lasso3.coef_\n",
    "y_test_pred = lasso3.predict(X_test_rfe3)\n",
    "\n",
    "print('The R2 Score of the model on the test dataset is',r2_score(y_test, y_test_pred))\n",
    "print('The MSE of the model on the test dataset is', mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "lasso3_coeff = pd.DataFrame(np.atleast_2d(lasso3_coef),columns=X_train_rfe3.columns)\n",
    "lasso3_coeff = lasso3_coeff.T\n",
    "lasso3_coeff.rename(columns={0: 'Lasso Co-Efficient'},inplace=True)\n",
    "lasso3_coeff.sort_values(by=['Lasso Co-Efficient'], ascending=False,inplace=True)\n",
    "\n",
    "print('\\n The most important predictor variables are as follows:')\n",
    "lasso3_coeff.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92d38aabcec263d45a20a3a93fd1e7269cea1af1b5c86589b9e3756a9e14658c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
